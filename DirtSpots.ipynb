{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DirtSpots.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDtCexYugguL"
      },
      "source": [
        "#### Install pycocotools, the version by default in Colab has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrmqHPAVgdGJ",
        "outputId": "325e1ef7-408a-40d6-f3f9-d02741eeae58"
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install cython\n",
        "pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (0.29.22)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-hf8k6cs8\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-hf8k6cs8\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (54.1.2)\n",
            "Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (0.29.22)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools==2.0) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools==2.0) (1.15.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp37-cp37m-linux_x86_64.whl size=263919 sha256=98a1b3c60517de7f81f46262c167e940c5389728e1c3f838dc11a9581bde39e1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4bl1znvv/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built pycocotools\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sd4jlGp2eLm"
      },
      "source": [
        "## Defining the Dataset\n",
        "\n",
        "\n",
        "* image: a PIL Image of size (H, W)\n",
        "* target: a dict containing the following fields\n",
        "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
        "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
        "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
        "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
        "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0rqK-A3Nbl"
      },
      "source": [
        "### Writing a custom dataset for Penn-Fudan\n",
        "\n",
        "First, let's download and extract the data, present in a zip file at https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t4TBwhHTdkd"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# download the Penn-Fudan dataset\n",
        "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
        "# extract it in the current folder\n",
        "unzip PennFudanPed.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfwuU-jI3j93"
      },
      "source": [
        "Let's have a look at the dataset and how it is layed down.\n",
        "\n",
        "The data is structured as follows\n",
        "```\n",
        "PennFudanPed/\n",
        "  PedMasks/\n",
        "    FudanPed00001_mask.png\n",
        "    FudanPed00002_mask.png\n",
        "    FudanPed00003_mask.png\n",
        "    FudanPed00004_mask.png\n",
        "    ...\n",
        "  PNGImages/\n",
        "    FudanPed00001.png\n",
        "    FudanPed00002.png\n",
        "    FudanPed00003.png\n",
        "    FudanPed00004.png\n",
        "```\n",
        "\n",
        "Here is one example of an image in the dataset, with its corresponding instance segmentation mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wguwob6niLwI"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "class ImageDirtier():\n",
        "  def __init__(self,number_of_spots,minimal_size,maximal_size):\n",
        "    self.number_of_spots = number_of_spots\n",
        "    self.minimal_size = minimal_size\n",
        "    self.maximal_size = maximal_size\n",
        "    self.minimal_transparency = 0.8\n",
        "    self.maximal_transparency = 0.9\n",
        "    self.lod = 7\n",
        "\n",
        "  def to_radians(self,angle):\n",
        "    return angle * np.pi / 180.0\n",
        "\n",
        "  def point_on_circle(self,center_point,angle,radius):\n",
        "    x = center_point[0] + radius * np.cos(self.to_radians(angle))\n",
        "    y = center_point[1] + radius * np.sin(self.to_radians(angle))\n",
        "    return np.array([x,y])\n",
        "\n",
        "  def minmax(self,number,limit):\n",
        "    return max(0,min(number,limit))\n",
        "  \n",
        "  def generate_polygons(self,width,height):\n",
        "    polygons = np.zeros((self.number_of_spots, self.lod, 3))\n",
        "    for i in range(self.number_of_spots):\n",
        "      angle = 0\n",
        "      move_angle = 360 / self.lod\n",
        "      main_radius = np.random.uniform(self.minimal_size, self.maximal_size)\n",
        "      center_point = np.random.randint([0 + main_radius, 0 + main_radius],[width - main_radius, height - main_radius])\n",
        "      bonus = 0\n",
        "      for j in range(self.lod):\n",
        "        move = np.random.randint(0,move_angle + bonus)\n",
        "        diff_radius = np.random.uniform(main_radius / 6,main_radius / 2)\n",
        "        bonus += move_angle - move\n",
        "        angle += move\n",
        "        new_point = self.point_on_circle(center_point, angle, main_radius - diff_radius)\n",
        "        polygons[i][j] = np.array([self.minmax(new_point[0], width),self.minmax(new_point[1], height), 1],dtype = np.int)\n",
        "    return polygons\n",
        "  \n",
        "  def parse_polygons(self,polygons):\n",
        "    polygons_contours = []\n",
        "    for i in range(polygons.shape[0]):\n",
        "      polygons_contours.append([])\n",
        "      transformed_polygon = []\n",
        "      for j in range(polygons.shape[1]):\n",
        "        transformed_polygon.append([polygons[i][j][0], polygons[i][j][1]])\n",
        "      polygons_contours[i].append(np.array(transformed_polygon, dtype = np.int))\n",
        "    return polygons_contours\n",
        "\n",
        "  def apply_spots(self,image, contours):\n",
        "    mask = np.zeros(image.shape[:2])\n",
        "    img2 = np.copy(image)\n",
        "    for i, contour in enumerate(contours):\n",
        "      cv2.drawContours(mask, contour, 0, 255*(i+1), -1)\n",
        "      color = np.random.uniform(0,16)\n",
        "      cv2.drawContours(img2, contour, 0, (color, color, color), -1)\n",
        "    transparency = np.random.uniform(self.minimal_transparency, self.maximal_transparency)\n",
        "    img2 = cv2.addWeighted(image, transparency, img2, 1 - transparency, 0, img2)\n",
        "    return img2,mask/255.\n",
        "  \n",
        "  def apply(self,image):\n",
        "    polygons = self.generate_polygons(image.shape[0], image.shape[1])\n",
        "    polygons_contours = self.parse_polygons(polygons)\n",
        "    return self.apply_spots(image, polygons_contours)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDjuVFgexFfh"
      },
      "source": [
        "from PIL import Image\n",
        "Image.open('PennFudanPed/PNGImages/FudanPed00001.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XttWa4fw72vT"
      },
      "source": [
        "img_dirtier = ImageDirtier(15,5,10)\n",
        "image = Image.open('PennFudanPed/PNGImages/FudanPed00001.png')\n",
        "img, mask = img_dirtier.apply(np.array(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwAgM1gx9Fqw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize = (20,10))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "plt.imshow(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Ee5NV54Dmj"
      },
      "source": [
        "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let's write a `torch.utils.data.Dataset` class for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTgWtixZTs3X"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "      self.root = root\n",
        "      self.transforms = transforms\n",
        "      # load all image files, sorting them to\n",
        "      # ensure that they are aligned\n",
        "      self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "      self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "      self.dirt_spots_creator = ImageDirtier(number_of_spots = 15, minimal_size = 5, maximal_size = 10)\n",
        "\n",
        "    def merge_masks(self, under_mask, upper_mask, start):\n",
        "      ret_mat = np.where(upper_mask != 0,upper_mask,under_mask)\n",
        "      return np.where(ret_mat == start,0,ret_mat)\n",
        "\n",
        "    def reduce_mask(self, mask):\n",
        "      obj_ids = np.unique(mask)\n",
        "      obj_ids = obj_ids[1:]\n",
        "      num_obj = len(obj_ids)\n",
        "      masks = mask == obj_ids[:,None,None]\n",
        "      for i in range(num_obj):\n",
        "        pos = np.where(masks[i])\n",
        "        xmin = np.min(pos[1])\n",
        "        xmax = np.max(pos[1])\n",
        "        ymin = np.min(pos[0])\n",
        "        ymax = np.max(pos[0])\n",
        "        area = (ymax - ymin) * (xmax - xmin)\n",
        "        if(area <= 15):\n",
        "          mask[np.where(mask == obj_ids[i])] = 0\n",
        "      return mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # load images ad masks\n",
        "      img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "      mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "      img = Image.open(img_path).convert(\"RGB\")\n",
        "      # note that we haven't converted the mask to RGB,\n",
        "      # because each color corresponds to a different instance\n",
        "      # with 0 being background\n",
        "      mask = Image.open(mask_path)\n",
        "      img, mask = self.dirt_spots_creator.apply(np.array(img))\n",
        "\n",
        "      mask = np.array(mask)\n",
        "      # mask = merge_masks(motion_mask_to_label,mask,number_of_polygons+1) in case of more than 1 class\n",
        "      mask = self.reduce_mask(mask)\n",
        "      # instances are encoded as different colors\n",
        "      obj_ids = np.unique(mask)\n",
        "      # first id is the background, so remove it\n",
        "      obj_ids = obj_ids[1:]\n",
        "\n",
        "      # split the color-encoded mask into a set\n",
        "      # of binary masks\n",
        "      masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "      # get bounding box coordinates for each mask\n",
        "      num_objs = len(obj_ids)\n",
        "      boxes = []\n",
        "      for i in range(num_objs):\n",
        "        pos = np.where(masks[i])\n",
        "        xmin = np.min(pos[1])\n",
        "        xmax = np.max(pos[1])\n",
        "        ymin = np.min(pos[0])\n",
        "        ymax = np.max(pos[0])\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "      boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "      # there is only one class\n",
        "      labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "      masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "      image_id = torch.tensor([idx])\n",
        "      area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "      # suppose all instances are not crowd\n",
        "      iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "      target = {}\n",
        "      target[\"boxes\"] = boxes\n",
        "      target[\"labels\"] = labels\n",
        "      target[\"masks\"] = masks\n",
        "      target[\"image_id\"] = image_id\n",
        "      target[\"area\"] = area\n",
        "      target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "      if self.transforms is not None:\n",
        "        img, target = self.transforms(img, target)\n",
        "\n",
        "      return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6f3ZOTJ4Km9"
      },
      "source": [
        "That's all for the dataset. Let's see how the outputs are structured for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEARO4B_ye0s"
      },
      "source": [
        "dataset = PennFudanDataset('PennFudanPed/')\n",
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWOhcsir9Ahx"
      },
      "source": [
        "So we can see that by default, the dataset returns a `PIL.Image` and a dictionary\n",
        "containing several fields, including `boxes`, `labels` and `masks`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoAEkUgn4uEq"
      },
      "source": [
        "## Defining your model\n",
        "Faster R-CNN\n",
        "\n",
        "![Faster R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image03.png)\n",
        "\n",
        "Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.\n",
        "\n",
        "![Mask R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image04.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjNHjVMOyYlH"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "      \n",
        "def get_instance_segmentation_model(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
        "                                                       hidden_layer,\n",
        "                                                       num_classes)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WXLwePV5ieP"
      },
      "source": [
        "That's it, this will make model be ready to be trained and evaluated on our custom dataset.\n",
        "\n",
        "## Training and evaluation functions\n",
        "\n",
        "In `references/detection/,` there is a number of helper functions to simplify training and evaluating detection models.\n",
        "Here, I will use `references/detection/engine.py`, `references/detection/utils.py` and `references/detection/transforms.py`.\n",
        "\n",
        "Let's copy those files (and their dependencies) in here so that they are available in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYDb7PBw55b-"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u9e_pdv54nG"
      },
      "source": [
        "\n",
        "\n",
        "Let's write some helper functions for data augmentation / transformation, which leverages the functions in `refereces/detection` that we have just copied:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l79ivkwKy357"
      },
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # I will maybe add more of them\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzCLqiZk-sjf"
      },
      "source": [
        "#### Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "### Putting everything together\n",
        "\n",
        "We now have the dataset class, the models and the data transforms. Let's instantiate them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5dGaIezze3y"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5yvZUprj4ZN"
      },
      "source": [
        "Now let's instantiate the model and the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoenkCj18C4h"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "\n",
        "# get the model using our helper function\n",
        "model = get_instance_segmentation_model(num_classes)\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by\n",
        "# 10x every 3 epochs\n",
        "# here I will probably add option to use some evolutionary algorithm to optimize this params, training speed will drop\n",
        "# a lot if we use it but results should be better\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAd56lt4kDxc"
      },
      "source": [
        "And now let's train the model for 10 epochs, evaluating at the end of every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "at-h4OWK0aoc"
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6mYGFLxkO8F"
      },
      "source": [
        "Now that training has finished, let's have a look at what it actually predicts in a test image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHwIdxH76uPj"
      },
      "source": [
        "# pick one image from the test set\n",
        "img, _ = dataset_test[1]\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    prediction = model([img.to(device)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr0U5aBJKNUh"
      },
      "source": [
        "plt.imshow(img.permute(1,2,0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmN602iKsuey"
      },
      "source": [
        "Printing the prediction shows that we have a list of dictionaries. Each element of the list corresponds to a different image. As we have a single image, there is a single dictionary in the list.\n",
        "The dictionary contains the predictions for the image we passed. In this case, we can see that it contains `boxes`, `labels`, `masks` and `scores` as fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkmb3qUu6zw3"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwT21rzotFbH"
      },
      "source": [
        "Let's inspect the image and the predicted segmentation masks.\n",
        "\n",
        "For that, we need to convert the image, which has been rescaled to 0-1 and had the channels flipped so that we have it in `[C, H, W]` format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpqN9t1u7B2J"
      },
      "source": [
        "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M58J3O9OtT1G"
      },
      "source": [
        "And let's now visualize the top predicted segmentation mask. The masks are predicted as `[N, 1, H, W]`, where `N` is the number of predictions, and are probability maps between 0-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v5S3bm07SO1"
      },
      "source": [
        "for img in range(prediction[0]['masks'].shape[0]):\n",
        "  plt.imshow(prediction[0]['masks'][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j385icSlHLUH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}